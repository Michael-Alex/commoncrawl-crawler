This is the primary repository for the services & map-reduce jobs used to produce the 
CommonCrawl web corpus. 

## Tree Structure 

* org.commoncrawl.async  				- Utility code used to build Async server.
* org.commoncrawl.hadoop.io 			- ARCInputFormat and related classes.
* org.commoncrawl.hadoop.mergeutils 	- Support for merge-sorts outside the context of a Hadoop job.
* org.commoncrawl.hadoop.template  	    - Sample Hadoop Job.
* org.commoncrawl.io 					- CommonCrawl IO library used by crawlers.
* org.commoncrawl.mapred				- Root for all MapReduce jobs. Also contains data structure definitions shared across jobs (database.jr).
* org.commoncrawl.mapred.ec2.parser	    - Code used to generate ARCFiles and intermediate data on EC2 using EMR.
* org.commoncrawl.mapred.ec2.postprocess.deduper  - Code to support a parallel dedupe using a 64bit Simhash.
* org.commoncrawl.mapred.ec2.postprocess.linkCollector - Code to merge metadata generated by the parser job.
* org.commoncrawl.mapred.pipelineV3		- The start of the new Nutch Free map-reduce pipeline used to process crawl metadata and generate new crawl lists.
* org.commoncrawl.mapred.segmenter      - Support code used to generate Crawl Segment (URL lists consumed by the crawlers).
* org.commoncrawl.protocol  			- Shared data structure and enum definitions (generated).
* org.commoncrawl.rpc  					- CommonCrawl RPC library used to build distributed systems.
* org.commoncrawl.server  				- CommonCrawl Server base class used by various services.
* org.commoncrawl.service				- All long lived processes in the CommonCrawl system are house under this directory.
* org.commoncrawl.service.crawler		- The crawler long running process (Consumes Crawl Lists, writes content to HDFS).
* org.commoncrawl.service.crawlhistory  - A service that manages a crawler's crawl state in a BloomFilter.
* org.commoncrawl.service.directory     - A barebones service used to store and subscribe to lists via a path.
* org.commoncrawl.service.dns		  	- CommonCrawl DNS Service (used by crawlers to queue up DNS requests).
* org.commoncrawl.service.listcrawler   - A different type of list crawler that supports dynamic uploading a crawling of very large lists of URLS.
* org.commoncrawl.service.pagerank	    - PageRank Master / Slave implementations (and related code) used to compute PageRank across the graph.
* org.commoncrawl.service.parser		- The beginnings of a distributed parser service that Crawlers can use to do on demand link extraction.
* org.commoncrawl.service.queryserver   - The (deprecated) crawl metadata service.
* org.commoncrawl.service.statscollector - Service that receives crawl stats.  
 


